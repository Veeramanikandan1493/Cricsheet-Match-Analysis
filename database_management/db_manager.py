import logging
import os

import pandas as pd
import toml
from sqlalchemy import create_engine
from tqdm import tqdm

# Set up logger for the module.
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    handler.setFormatter(formatter)
    logger.addHandler(handler)


class DBManager:
    def __init__(self, connection_string=None, processed_folder=None):
        """
        Initializes the DBManager with a connection to MySQL and sets the folder path for processed CSV files.
        :param connection_string: SQLAlchemy connection string for MySQL.
        :param processed_folder: Folder containing CSV files generated by the DataTransformer.
        """
        # Determine project root (one directory up from this module's location).
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
        logger.info(f"Project root determined as: {project_root}")

        # Set default connection string if none provided.
        # Update with your MySQL credentials and database name.
        if not connection_string:
            config_path = os.path.join(project_root, "config.toml")
            config = toml.load(config_path)
            connection_string = config["db"]["connection_string"]
        self.connection_string = connection_string

        try:
            self.engine = create_engine(connection_string, echo=False)
            logger.info("SQLAlchemy engine created successfully.")
        except Exception as e:
            logger.error(f"Failed to create SQLAlchemy engine: {e}")
            raise

        # Set default processed folder if not provided.
        if not processed_folder:
            processed_folder = os.path.join(project_root, "data", "processed")
        self.processed_folder = processed_folder

    def create_tables(self):
        """
        Creates database tables if they do not exist.
        In this example, we rely on pandas.to_sql (which auto-creates the tables).
        You can extend this method to run explicit DDL statements if needed.
        """
        logger.info("Table creation will be handled automatically by pandas.to_sql().")

    def insert_data(self):
        """
        Inserts data from CSV files into MySQL tables using batch inserts.
        Four CSV files are expected in the processed folder:
            - test_matches.csv
            - odi_matches.csv
            - t20_matches.csv
            - deliveries.csv
        Each CSV file is loaded into a DataFrame and then inserted into its corresponding table
        in batches using tqdm to monitor progress.
        """
        csv_files = {
            "test_matches": "test_matches.csv",
            "odi_matches": "odi_matches.csv",
            "t20_matches": "t20_matches.csv",
            "deliveries": "deliveries.csv",
        }
        batch_size = 50000  # Set batch size per DataFrame
        for table_name, file_name in csv_files.items():
            file_path = os.path.join(self.processed_folder, file_name)
            try:
                df = pd.read_csv(file_path)
                logger.info(f"Inserting data from {file_path} into table '{table_name}' in batches of {batch_size}...")
                total_rows = len(df)
                num_batches = (total_rows // batch_size) + (1 if total_rows % batch_size != 0 else 0)
                for i in tqdm(range(num_batches), desc=f"Inserting {table_name} batches"):
                    start = i * batch_size
                    end = start + batch_size
                    chunk = df.iloc[start:end]
                    # For the first batch, replace existing data, then append subsequent batches.
                    if i == 0:
                        chunk.to_sql(table_name, self.engine, if_exists='replace', index=False)
                    else:
                        chunk.to_sql(table_name, self.engine, if_exists='append', index=False)
                logger.info(f"Data inserted into '{table_name}' successfully.")
            except Exception as e:
                logger.error(f"Error inserting data from {file_path} into table '{table_name}': {e}")

    def run(self):
        """
        Runs the table creation (if necessary) and data insertion processes.
        """
        self.create_tables()
        self.insert_data()


if __name__ == "__main__":
    # Instantiate DBManager (update connection_string as needed)
    db_manager = DBManager()
    try:
        db_manager.run()
    except Exception as e:
        logger.error(f"An error occurred during database operations: {e}")
